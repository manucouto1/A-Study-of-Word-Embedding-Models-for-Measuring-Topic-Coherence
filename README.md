# A-Study-of-Word-Embedding-Models-for-Measuring-Topic-Coherence
This repository provides code for evaluating topic coherence using word embeddings. It compares models like Word2Vec, FastText, GloVe, BERT, RoBERTa, ALBERT, and MPNET, showing that embedding-based metrics can outperform traditional methods in assessing the quality of topic model outputs.
